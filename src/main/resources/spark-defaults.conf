# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
 spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.driver.memory              16g
spark.executor.memory            64g

# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
# spark.cores.max         19


# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.kryo.registrator           org.apache.spark.graphx.GraphKryoRegistrator
# spark.eventLog.enabled           true
# spark.eventLog.dir               /var/tmp/store-w20/eventLog
# spark.local.dir                         /var/tmp/store-w20/local
# spark.log.Conf                          true
# spark.eventLog.compress                 false
# spark.ui.retainedStages                 10000
#spark.default.parallelism	 12

##to get around the kryo serializer buffer limit problem
# spark.kryoserializer.buffer.max.mb 1024
#spark.executor.extraClassPath   /usr/local/hadoop/extras/jblas-master/target/jblas-1.2.4-SNAPSHOT.jar
# spark.executor.extraLibraryPath /usr/lib64

# spark.sql.dialect sql
# hiveql parser is more robust and allows access to hive udfs.
##spark.sql.dialect hiveql

# this is needed when creating files/tables and switching between spark-scala-shell,
# spark-sql-shell, and jdbc such that they can all read/write the parquet files correctly
# to share among all shell flavors.
# spark.sql.parquet.binaryAsString true
##spark.sql.hive.version=0.12.0

#/usr/local/hadoop/hadoop-2.5.1/lib/native
#spark.executor.extraClassPath   /usr/local/hadoop/extras/sparkjars/netlib-native_system-linux-x86_64-1.1-natives.jar
#spark.executor.extraClassPath   /usr/local/hadoop/extras/sparkjars/
##spark.executor.extraClassPath   /usr/local/hadoop/spark-1.1.0/assembly/target/scala-2.10/spark-assembly-1.1.0-hadoop2.4.0.jar
##spark.executor.extraClassPath   /usr/local/hadoop/spark-1.1.0/assembly/target/scala-2.10
#spark.executor.extraClassPath   /usr/local/hadoop/spark-1.1.0/assembly/target/scala-2.10/spark-assembly-1.1.0-hadoop2.4.0.jar
# spark.executor.extraClassPath   /usr/local/hadoop/extras/sparkjars/spark-assembly-1.1.0-hadoop2.4.0.jar
#spark.executor.extraClassPath   /usr/local/hadoop/extras/sparkjars/netlib-native_system-linux-x86_64-1.1-natives.jar

# spark.driver.maxResultSize   0